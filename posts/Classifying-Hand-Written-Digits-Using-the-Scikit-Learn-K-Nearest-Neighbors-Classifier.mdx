---
title: Classifying Hand Written Digits Using the Scikit-Learn K-Nearest Neighbors Classifier
tags:
  - python
  - machine learning
  - programming
  - data science
date: 2022-09-10
excerpt: [excerpt]
---

## Structuring the Data

Begin by importing and loading the digits dataset from 
[Scikit-learn](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_digits.html)

```python
from sklearn.datasets import load_digits

digits = load_digits()
```

As you'd expect this dataset contains 10 classes, or targets, one for each
base-10 numeral. Each class contains approximately 180 samples.

Each sample has 64 total dimensions, or attributes, containing a value from
0-16. These attributes form an 8x8 bitmap matrix where the value
represents a grayscale value between white and black.


```python
print("target_names:", digits.target_names)
# output: target_names: [0 1 2 3 4 5 6 7 8 9]

print("data shape:", digits.data.shape)
# output: data shape: (1797, 64)

print("images shape:", digits.images.shape)
# output: images shape: (1797, 8, 8)
```

We can use matplotlib to visualize the data.

```python
import matplotlib.pyplot as plt

_, axes = plt.subplots(nrows=1, ncols=4, figsize=(10,3))
for ax, image, label in zip(axes, digits.images, digits.target):
    ax.set_axis_off()
    ax.imshow(image, cmap=plt.cm.gray_r, interpolation="nearest")
    ax.set_title("Training: %i" % label)
```

<Image src="/posts/Classifying-Hand-Written-Digits-Using-the-Scikit-Learn-K-Nearest-Neighbors-Classifier/imagePlot.png" 
    width={825*.55}
    height={234*.55}
/>

<br/>
<br/>

We'll add the targets to our DataFrame for the purpose of visualization.

```python
df["target"] = digits.target

# display the frist 10 rows
df.head(10)
```

<Image src="/posts/Classifying-Hand-Written-Digits-Using-the-Scikit-Learn-K-Nearest-Neighbors-Classifier/dataFrame.png" 
    width={779*.6}
    height={492*.6}
/>

<br/>
<br/>

## Using the Classifier

Import the [classifier](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html)
and the [train_test_split](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html)
method. The train_test_split method sections the data into training and testing
data matricies ($X$) and target vectors ($y$). $X$ are your independent
varables and $y$ is the dependent variable. 

Create $X$ and $y$ then pass them into train_test_split to get your training
and testing data. You'll need to specify how much of $X$ you'd like to reserve
for testing. Here I chose 20%, that's typically a good place to start.

The random state argument will shuffle $X$ to randomize the data before
splitting. It's not necessary here but I included it for demonstration 
purposes.

Make sure you drop any
unnecessary columns from the DataFrame (just the target column in this case)
while creating $X$.

```python
from sklearn.model_selection import train_test_split
from sklearn.neighbors import KNeighborsClassifier

X = df.drop(['target'], axis='columns')
y = df.target

X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)

print("X_train shape:", X_train.shape)
# output: X_train shape: (1437, 64)

print("y_train shape:", y_train.shape)
# output: y_train shape: (1437,)
```

_Sidenote: Why is the standard notation to use a capital $X$ and lower case $y$?
In linear algebra, it's convention to use capital Latin letters to represent
matrices, and lower case Latin letters to represent vectors._

Now we finally train and use the classifier.

The n_neighbors parameter is the $k$ value. You can determine the optimal $k$
value via trial and error or use a sklearn's gridsearchcv method.

_A $k$ value that's too small is susceptible to outliers, a $k$ value that's
too large risks being outvoted by the wrong selection group. In general a smaller
$k$ value is more optimal for structured data sets (such as this one) and a larger
$k$ value is more optimal for noisy data sets._

```python
knn = KNeighborsClassifier(n_neighbors=7)

# train the classifer
knn.fit(X_train, y_train)

# use the classifier and determine it's accuracy
print("accuracy:", knn.score(X_test, y_test))
# output: accuracy: 0.9888888888888889
```

The following are a few different reports that can be used to visualize the
classifier's accuracy.

To read a [confusion matrix](https://en.wikipedia.org/wiki/Confusion_matrix),
see where a given row and column intersect. This will tell you how many times,
if any, a test sample (the row value) was predicted (the col value)
correctly or incorrectly. For example, _7_ was predicted _correctly_ 33 times 
and _incorrectly_ 1 time, when it was classified as a 9 instead.

```python
from sklearn.metrics import confusion_matrix

y_pred = knn.predict(X_test)
cm = confusion_matrix(y_test, y_pred)

%matplotlib inline
import matplotlib.pyplot as plt
import seaborn as sn

plt.figure(figsize=(7,5))
sn.heatmap(cm, annot=True)
plt.xlabel('Predicted')
plt.ylabel('Truth')
```

<Image src="/posts/Classifying-Hand-Written-Digits-Using-the-Scikit-Learn-K-Nearest-Neighbors-Classifier/confusionMatrix.png" 
    width={637*.6}
    height={493*.6}
/>

<br/>
<br/>

And here's an sklearn [classification report](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.classification_report.html)

```python
from sklearn.metrics import classification_report

print(classification_report(y_test, y_pred))
```

<Image src="/posts/Classifying-Hand-Written-Digits-Using-the-Scikit-Learn-K-Nearest-Neighbors-Classifier/classificationReport.png" 
    width={667*.6}
    height={422*.6}
/>
