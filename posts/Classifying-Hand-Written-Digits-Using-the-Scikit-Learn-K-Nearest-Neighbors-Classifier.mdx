---
title: Classifying Hand Written Digits Using the Scikit-Learn K-Nearest Neighbors Classifier
tags:
  - python
  - machine learning
  - programming
  - data science
date: 2022-08-20
excerpt: [excerpt]
---

Begin by importing and loading the digits dataset from 
[Scikit-learn](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_digits.html)

```python
from sklearn.datasets import load_digits

digits = load_digits()
```

As you'd expect this dataset contains 10 classes, or targets, one for each
base-10 numeral. Each class contains approximately 180 samples.

Each sample has 64 total dimensions, or attributes, containing a value from
0-16. These attributes form an 8x8 bitmap matrix where the value
represents a grayscale value between white and black.


```python
print("target_names:", digits.target_names)
# output: target_names: [0 1 2 3 4 5 6 7 8 9]

print("data shape:", digits.data.shape)
# output: data shape: (1797, 64)

print("images shape:", digits.images.shape)
# output: images shape: (1797, 8, 8)
```

We can use matplotlib to visualize the images of each target.

```python
import matplotlib.pyplot as plt

_, axes = plt.subplots(nrows=1, ncols=4, figsize=(10,3))
for ax, image, label in zip(axes, digits.images, digits.target):
    ax.set_axis_off()
    ax.imshow(image, cmap=plt.cm.gray_r, interpolation="nearest")
    ax.set_title("Training: %i" % label)
```

<Image src="/posts/Classifying-Hand-Written-Digits-Using-the-Scikit-Learn-K-Nearest-Neighbors-Classifier/imagePlot.png" 
    width={825*.55}
    height={234*.55}
/>

<br/>
<br/>

In order to create our testing and training data we'll need to initialize a
[DataFrame](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.html),
the primary data structure used by pandas. A DataFrame only accepts
2-Dimensional data, our's is 3-Diminsonal so we'll have to flatten it first.

```python
n_samples = len(digits.images)
data = digits.images.reshape((n_samples, -1))

print("flattened attributes:", data.shape)
# output: flattened attributes: (1797, 64)

import pandas as pd

df = pd.DataFrame(data, columns=digits.feature_names)
```

We'll add the targets to our DataFrame for the purposes of visualization.

```python
df["target"] = digits.target

# display the frist 10 rows
df.head(11)
```

<Image src="/posts/Classifying-Hand-Written-Digits-Using-the-Scikit-Learn-K-Nearest-Neighbors-Classifier/dataFrame.png" 
    width={779*.6}
    height={492*.6}
/>

<br/>
<br/>

Import the [classifier](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html)
and the [train_test_split](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html)
method. The train_test_split method sections the data into training and testing
data matricies ($X$) and target vectors ($y$). When you create these, drop the
unnecessary columns from the DataFrame first (just the target column in this case).

```python
from sklearn.model_selection import train_test_split
from sklearn.neighbors import KNeighborsClassifier

X = df.drop(['target'], axis='columns')
y = df.target

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

print("X_train shape:", X_train.shape)
# output: X_train shape: (1437, 64)

print("y_train shape:", y_train.shape)
# output: y_train shape: (1437,)
```

Sidenote: Why is the standard notation to use a capital $X$ and lower case $y$?
In linear algebra, it's convention to use capital Latin letters to represent
matrices, and lower case Latin letters to represent vectors.

Now we finally train and use the classifier.

The n_neighbors parameter is the $k$ value. You can determine the optimal $k$
value via trial and error or use a sklearn's gridsearchcv method.

A $k$ value that's too small is susceptible to outliers, a $k$ value that's
too large risks being outvoted by the wrong selection group. In general a smaller
$k$ value is more optimal for structured data sets (such as this one) and a larger
$k$ value is more optimal for noisy data sets.

```python
knn = KNeighborsClassifier(n_neighbors=7)

# train the classifer
knn.fit(X_train, y_train)

# use the classifier and determine it's accuracy
print("accuracy:", knn.score(X_test, y_test))
# output: accuracy: 0.9888888888888889
```

The following is a few different reports that can be used to visualize the
classifier's accuracy.

To read a [confusion matrix](https://en.wikipedia.org/wiki/Confusion_matrix),
see where a given row and column intersect. This will tell you how many times,
if any, a test sample (the row value) was predicted (the col value) incorrectly.

```python
from sklearn.metrics import confusion_matrix

y_pred = knn.predict(X_test)
cm = confusion_matrix(y_test, y_pred)

%matplotlib inline
import matplotlib.pyplot as plt
import seaborn as sn

plt.figure(figsize=(7,5))
sn.heatmap(cm, annot=True)
plt.xlabel('Predicted')
plt.ylabel('Truth')
```

<Image src="/posts/Classifying-Hand-Written-Digits-Using-the-Scikit-Learn-K-Nearest-Neighbors-Classifier/confusionMatrix.png" 
    width={637*.6}
    height={493*.6}
/>

<br/>
<br/>

```python
from sklearn.metrics import classification_report

print(classification_report(y_test, y_pred))
```

<Image src="/posts/Classifying-Hand-Written-Digits-Using-the-Scikit-Learn-K-Nearest-Neighbors-Classifier/classificationReport.png" 
    width={667*.6}
    height={422*.6}
/>
