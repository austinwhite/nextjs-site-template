---
title: The Anatomy of an Interpreter
tags:
  - tag1
  - tag2
  - tag3
  - tag4
date: 2022-08-09
excerpt: put the exerpt here
---

<small>*The following is summarized from the book: Crafting Interpreters by 
<a href="https://stuffwithstuff.com" target="_blank">Robert Nystrom</a>*</small>

<Image src="/posts/tmpname/programmingLangs.svg" width={3440} height={1440} />

## Scanning
The first job of the interpreter is scanning, also known as lexing (or 
lexical analysis). The scanner injests source code as a linear stream of
characters and groups them together into what's reffered to a 'token'.
Some tokens are a single character (i.e. \{ and ,) while other can be several
character long, such as string literals ("hi"), numbers (123), and
identifiers (min). 

Some characters from the source code don't have any
significace to the scanner such as whitespace and comments. These things
are only usefull to the programmer so the interpreter will typically 
ignore them.

```c
float avg = (min + max) / 2;
```

The statement above would result in the following tokens:
- float
- avg
- =
- (
- min
- \+
- max
- )
- 2
- ;

Scanning is a ubiqitous task done for every new language that pops up, no 
matter how small. Open source tools have been created that can take a 
predefined grammer and output a fully functioning scanner. See 
https://github.com/westes/flex

## Parsing
The parser give the languge it's sytax using rules outlines in the laguges
'grammar'. Much like in linguistic languages, the grammer dictats how words
(tokens for us) link together to form coherent expressions and statements.

The parser takes a flat sequnce of tokens, and constructs them into an
Abstract Syntax Tree (AST).

<Image src="/posts/tmpname/AST.svg" width={3440} height={1440} />

During the construction of this AST, the interpreter will find and report
any syntax errors that exist.

## Static analysis
At this point we know the syntactic structure of the code but we don't 
know much more.

For example, for the expression a + b, we know that we're adding a and b,
but what does a and b refer to? Which scope do they belong to a?

The first part of analysis is called 'binding'. For each identifier we find
out where it's defined, then we can connect the name to it's defininition.
This is also were we resolve the scope of each identifier and ensure that
it's being respected, otherwise we'll throw an error.

In the case of a statically typed language, we'll also determine if the types
of the values being added support it. If not, we'll throw an error.

The definitinons and type are often stored back into the tree as 'attributes'.
Another method is to store the attributes in a seperate lookup table, where
the identifiers act as the keys. Howver the most powerful method is to 
transform the tree into an entirly new data structure, an Intermediate
representation. This is covered in the next section.

Everything up to this point is reffered as the language 'front end'. This 
is where all the language specifics are handled, the 'back end' is concerned 
with final archetecture where the code will run. In the time since these 
terms were coined new techniques have arison to make the backends job easier,
these techniques are groups together and called the 'middle end'.

## Intermediate representations
A compiler is essentially a pipeline where each successage stage's job is to 
make the next stage easier to implement. 

The Intermediate represention (IR) is part of the middle end. This is a 
standardized representaion of the data that allows you to mix and match
different front ends and back ends. There are many standards, see 
"control flow graph", "static single-assignment", "continuation-passing style",
and "three-address code".

Utlizing IR to mix and match front ends and back ends is how compilers
like GCC are able to support so many differnt languages and archetectures.

## Optimization
Transforming the code into an intermediate representatino also makes 
optimizatinos far easier to carry out.

for example, we could execture the following during compilatino and replace
every instance of pennyArea with a discrete value.

```c
float pennyArea = 3.14159 * (0.75 / 2) * (0.75 / 2);
```
would be replaced with:
```c
float pennyArea = 0.4417860938
```

## Code generation
In this step we generate the instruction primatives that will be run by the
CPU. This is officially the 'back end'. 

There are ultimatly two paths one can take, do you wan't to generate 
code to run nativly on a real CPU, or code that will run with in a container
on a virtual CPU? Compiling down to a real CPU results in the fastest program,
but this can be a highly involved task due to the complex (some might say
bloated) nature of modern CPU archetecures.

There's another option, compiling to bytecode. This is virtual machine code,
instead of instructions that will run on a real chip, this code is for a 
hypothecial machine. It's called bytecode becuase each instructino is usually
a single byte long. This code will then run on a virtual machine, the user
will be none the wiser.

## Virtual machine
There's no real chip that speaks in 'bytecode', you'll need to translate it.
There are again two paths to choose from, you can write a mini compiler for 
earch archetecure you'd like to support that converts the bytecode to native
code, you'll have to do this for every chip you support but you'll get to reuse 
the rest of the compiler pipeline accross all the machines you support. here
you're essentially using the bytecode as your IR.

The other option is wrte a 'virtual machine' (VM). This is a program which 
emulates the affomentinoed hypothetical chip. This option is significnalty
slower than runnign native code, however it comes with simplicity and 
portability.

## Runtime
It's time to run the compiled program. If we compiled down to native machine 
code, you can just tell your OS to run the program, if we went the VM route, 
you'll need to start up the VM and load the program into it.

In both cases there will be services that run alongside the executing program 
providing assistance where needed, i.e. garbage collector.
