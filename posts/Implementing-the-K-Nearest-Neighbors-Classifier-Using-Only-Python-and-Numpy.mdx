---
title: Implementing the K-Nearest Neighbors Classifier Using Only Python and Numpy
tags:
  - machine learning
  - programming
  - python
  - data science
date: 2022-08-21
excerpt: [excerpt]
---

## What is KNN?
The K-Nearest Neighbors algorithm is a supervised learning method used for 
classification and regression. The input is the $k$ closest training samples
in a data set. The output is class membership.

M amount of classification objects are mapped onto N-Dimensional euclidean space.
When new data is recieved the classifer will determine it's $k$ nearest neighbors.
Each neighbor casts a votes of $\frac{1}{k}$ for it's class. The new data is 
classifed as the class that cast a plurality of the votes.

### Chosing the Optimal $k$ Value
Chosing the correct $k$ value can make a large differnece in the accuracy of
your classifier. A $k$ value that's too small is susceptible to outliers, 
a $k$ value that's too large risks being outvoted by the wrong selection group.
In general a smaller $k$ value is more optimal for structured data sets
and a larger $k$ value is more optimal for noisy data sets.

You can find an optimal $k$ value via trial and error or use tools such as 
sklearns [GridSearchCV](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html)
method.

## The Underlying Math
Understanding the math behind this classifier isn't strictly neccessary but
if you're serious about this domain, it's worth your time to dig a little
deeper. A basic understanding of algebra and calculus is assumed here.

### The Euclidean Distance
The [euclidean distance](https://en.wikipedia.org/wiki/Euclidean_distance)
is the distance between 2 points in $N$-Dimensional
[euclidean space](https://en.wikipedia.org/wiki/Euclidean_space).
Imagine a sheet of graph paper where a horizontal line represents an $X$-Axis and an intersecting vertical
line represents a $Y$-Axis. This is 2-Dimensional euclydian space. Now imagine
a line perpendicular to the paper going through the exact point where the
$X$ and $Y$ axis intersect, this is the $Z$-Axis. Together $X$, $Y$, and $Z$ form
3-Dimensional euclidean space.

After this point direct visualazation becomes impossible however you can
continue adding additional dimensions indefinitly. An $N$-dimensional 
euclidian space is a euclidean space with $N$ dimensions where $N$ is any positive integer.

Euclidian distnace is defined as:
$$
d(p, q) =\sum_{i=0}^{n}\sqrt{(p_{i} - q_{i})^{2}}
$$

At it's core, this is really just a generalized form of the pythagorian
theorem: 
$
c^{2} = a^{2} + b^{2}
$
Isolate $c$ and you have: 
$
c = \sqrt{a^{2} + b^{2}}
$

<Image src="/posts/Implementing-the-K-Nearest-Neighbors-Classifier-Using-Only-Python-and-Numpy/pythagorian.png"
    width={550*.4}
    height={550*.4}
/>

Evaluating for $c$ in the above diagram you have:
$
c = \sqrt{(3)^{2} + (4)^{2}} \Longrightarrow \sqrt{25} \Longrightarrow \textbf{5}
$

If you were to plot the indicies that comprise the triangle above you can 
generalize the pythagorian theorem even further into what's refferd to as the
distance formula. 
$
d = \sqrt{(x_{2} - x_{1})^{2} + (y_{2} - y_{1})^{2}}
$

By differencing the corrisponding $x$ and $y$ points before squaring, you're 
effectivly translating the triangled indicies to the $X$ and $Y$ axis. After 
that the evaluation contunues as it did in the pythagorian theorem.

<Image src="/posts/Implementing-the-K-Nearest-Neighbors-Classifier-Using-Only-Python-and-Numpy/distancePlot.png"
    width={550*.55}
    height={550*.55}
/>

Solving for $d$ from the diagram above you get: 
$
d = \sqrt{(4 - 1)^{2} + (0 - -4)^{2}} \Longrightarrow \sqrt{(3)^{2} + (4)^{2}} \Longrightarrow \sqrt{25} \Longrightarrow \textbf{5}
$

Note: it doesn't matter which order to take the difference in the sections 
being squred. Squaring the values and then taking the square root has the effect 
of leaving any positive numbers as they are and taking the absolute value of
any negative number.

So far we've been evaluating distances in 2-Dimensions, we can generalize this 
to higher dimensions:

$
d(p,q) = \sqrt{(p_{1} - q_{1})^{2} + (p_{2} - q_{2})^{2} + ... + (p_{i} - q_{i})^{2} + ... + (p_{n} - q_{n})^{2}}
$

<br/>
Expresing this using sigma notation you get the euclidian distance forumala.
$$
d(p, q) =\sum_{i=0}^{n}\sqrt{(p_{i} - q_{i})^{2}}
$$

## Implementing KNN
Basic python knowledge is assumed here. Full source code can be found on my
[GitHub](https://github.com/austinwhite/k-nearest-neighbors)

This implementation follows the conventions of the
[scikit-learn KNeighborsClassifier](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html)

<br/>

In a file KNN.py, start by importing numpy and creating the class KNN.
Initialize the class with a k value.

```python
import numpy as nu

class KNN:
    __init__(self, k=3):
        self.k = k
```

The fit method load data into the classifier. The scikit-learn implementation 
is much more sophisticated, ours just stores $X$ and $y$ as instance variables.

```python
def fit(self, X, y):
    self.X_train = X
    self.y_train = y
```

All that remains is the predict method. It's here that the distances are 
calculated and the nearest neighbors cast their votes.

```python
def predict(self, X):
        y_pred = [self._predict(x) for x in X]
        return np.array(y_pred)

    def _predict(self, x):
        distances = [euclidean_distance(x, x_train) for x_train in self.X_train]

        k_indices = np.argsort(distances)[:self.k]
        k_nearest_labels = [self.y_train[i] for i in k_indices]

        most_common = Counter(k_nearest_labels).most_common(1)
        return most_common[0][0]
```

The _predict method calls euclidian_distance(), we'll need to define that as well.
It doesn't need to be contained within the class, you can add it to a utility 
file or just define it in the KNN.py file outside of the class.

```python
def euclidean_distance(p, q):
    return np.sqrt(np.sum((p - q)**2))
```

## Testing the Implementation
We'll use the iris dataset from scikit-learn.

```python
import numpy as np
from sklearn import datasets
from sklearn.model_selection import train_test_split
from KNN import KNN

# load the dataset
# create data features matrix and target vector
iris = datasets.load_iris()
X, y = iris.data, iris.target

# segment testing and training data
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=1234
)

# initialize classifier
knn = KNN(k=3)

# pass data into classifier
knn.fit(X_train, y_train)

# use classifier
predict = knn.predict(X_test)

# evaluate accuracy
acc = np.sum(predict == y_test) / len(y_test)

print(acc)
# output: 1.0
```
