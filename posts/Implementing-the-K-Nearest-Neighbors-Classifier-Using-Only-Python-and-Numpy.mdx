---
title: Implementing the K-Nearest Neighbors Classifier Using Only Python and Numpy
tags:
  - machine learning
  - programming
  - python
  - data science
date: 2022-09-20
excerpt: [excerpt]
---

## What is KNN?
The [K-Nearest Neighbors](https://en.wikipedia.org/wiki/K-nearest_neighbors_algorithm)
algorithm is a supervised learning method used for classification and 
regression. The input is the $\small k$ closest training samples in a data set.
The output is class membership.

$\small M$ amount of classification objects are mapped onto $\small N$-Dimensional Euclidean
space. When new data is received the classifier will determine its $\small k$ nearest
neighbors. Each neighbor casts a vote of $\frac{1}{k}$ for its class. The new
data is classified as the class that cast the plurality of votes.

### Chosing the Optimal $\small k$ Value
Choosing the correct $\small k$ value can make a large difference in the accuracy of
your classifier. A $\small k$ value that's too small is susceptible to outliers, 
a $\small k$ value that's too large risks being out voted by the wrong selection group.
In general a smaller $\small k$ value is more optimal for structured data sets
and a larger $\small k$ value is more optimal for noisy data sets.

You can find an optimal $\small k$ value via trial and error or use tools such as 
sklearns [GridSearchCV](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html)
method.

## The Underlying Math
Understanding the math behind this classifier isn't strictly necessary, but
if you're serious about this domain it's worth your time to dig a little
deeper. A basic understanding of algebra and calculus is assumed in this 
section. 

### The Euclidean Distance
The [Euclidean distance](https://en.wikipedia.org/wiki/Euclidean_distance)
is the distance between 2 points in $\small N$-Dimensional
[Euclidean space](https://en.wikipedia.org/wiki/Euclidean_space).
Imagine a sheet of graph paper where a horizontal line represents an $\small X$-Axis 
and an intersecting vertical line represents a $\small Y$-Axis. This is 2-Dimensional
Euclidean space. Now imagine a line perpendicular to the paper going through
the exact point where the $\small X$ and $\small Y$ axes intersect, this is the $\small Z$-Axis.
Together $\small X$, $\small Y$, and $\small Z$ form 3-Dimensional Euclidean space.

After this point direct visualization becomes impossible, however you can
continue adding additional dimensions indefinitely. An $\small N$-dimensional 
Euclidean space is a Euclidean space with $N$ dimensions where $\small N$ is any 
positive integer.

Euclidean distance is defined as:
$$
d(p, q) =\sum_{i=0}^{n}\sqrt{(p_{i} - q_{i})^{2}}
$$

At its core, this is really just a generalized form of the Pythagorean
theorem: 
$
\small c^{2} = a^{2} + b^{2}
$
Solve for $\small c$ and you have: 
$
\small c = \sqrt{a^{2} + b^{2}}
$

<Image 
    src="/posts/Implementing-the-K-Nearest-Neighbors-Classifier-Using-Only-Python-and-Numpy/pythagorian.png"
    width={550*.4}
    height={550*.4}
/>

Evaluating for $\small c$ in the above diagram you have:
$
\small c = \sqrt{(3)^{2} + (4)^{2}} \Longrightarrow \sqrt{25} \Longrightarrow \textbf{5}
$

If you were to plot the indices that comprise the triangle above, you can 
generalize the Pythagorean theorem even further into what's referred to as the
distance formula. 
$
\small d = \sqrt{(x_{2} - x_{1})^{2} + (y_{2} - y_{1})^{2}}
$

By taking the difference between the $\small x$ and $\small y$ points before squaring, 
you're effectively normalizing the points to the $\small X$ and $\small Y$ axes.
After that you'll continue applying the Pythagorean theorem as before.

<Image 
    src="/posts/Implementing-the-K-Nearest-Neighbors-Classifier-Using-Only-Python-and-Numpy/distancePlot.png"
    width={550*.55}
    height={550*.55}
/>

Solving for $\small d$ from the diagram above, you get: 
$
\small d = \sqrt{(4 - 1)^{2} + (0 - -4)^{2}} \Longrightarrow \sqrt{(3)^{2} + (4)^{2}} \Longrightarrow \sqrt{25} \Longrightarrow \textbf{5}
$

It doesn't matter which order you take the differences. Squaring the values 
and then taking the square root has the effect of leaving any positive numbers 
as they are and taking the absolute value of any negative numbers.

So far we've been evaluating distances in 2-Dimensions, we can generalize this 
to higher dimensions:

$
\small d(p,q) = \sqrt{(p_{1} - q_{1})^{2} + (p_{2} - q_{2})^{2} +...+ (p_{n} - q_{n})^{2}}
$

<br/>
Expressing this using sigma notation results in the Euclidian distance formula.
$$
\small d(p, q) =\sum_{i=0}^{n}\sqrt{(p_{i} - q_{i})^{2}}
$$

## Implementing KNN
Basic python knowledge is assumed moving forward. Full source code can be 
found on my [GitHub](https://github.com/austinwhite/k-nearest-neighbors)

This implementation follows the conventions of the
[Scikit-Learn KNeighborsClassifier](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html)

<br/>

In a file KNN.py, start by importing numpy and creating the class KNN.
Initialize the class with a k value.

```python
import numpy as nu

class KNN:
    __init__(self, k=3):
        self.k = k
```

The fit method loads data into the classifier. The Scikit-Learn implementation 
is much more sophisticated, ours just stores $\small X$ and $\small y$ as instance variables.

```python
def fit(self, X, y):
    self.X_train = X
    self.y_train = y
```

All that remains is the predict() method. It's here that the distances are 
calculated and the nearest neighbors cast their votes.

```python
def predict(self, X):
    y_pred = [self._predict(x) for x in X]
    return np.array(y_pred)

def _predict(self, x):
    distances = [euclidean_distance(x, x_train) for x_train in self.X_train]

    k_indices = np.argsort(distances)[:self.k]
    k_nearest_labels = [self.y_train[i] for i in k_indices]

    most_common = Counter(k_nearest_labels).most_common(1)
    return most_common[0][0]
```

The _predict method calls euclidian_distance(), we'll need to define that as
well. It doesn't need to be contained within the class, you can add it to a
utility file or just define it in the KNN.py file outside of the class.

```python
def euclidean_distance(p, q):
    return np.sqrt(np.sum((p - q)**2))
```

## Testing the Implementation
We'll use the iris dataset from Scikit-Learn.

```python
import numpy as np
from sklearn import datasets
from sklearn.model_selection import train_test_split
from KNN import KNN

# load the dataset
# create data features matrix and target vector
iris = datasets.load_iris()
X, y = iris.data, iris.target

# segment testing and training data
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=1234
)

# initialize classifier
knn = KNN(k=3)

# pass data into classifier
knn.fit(X_train, y_train)

# use classifier
predict = knn.predict(X_test)

# evaluate accuracy
acc = np.sum(predict == y_test) / len(y_test)

print(acc)
# output: 1.0
```
